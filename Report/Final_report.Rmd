---
output: pdf_document
---

  title: "Outliers in Time Series"
subtitle: "ST 565 Final Project (Winter 2016)"
author: |
  | Amir Azarbakht, Michael Dumelle,
| Camden Lopez & Tadesse Zemicheal
fontsize: 12pt
output: pdf_document

  
  ```{r, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE,
                      results = "hide", fig.height = 3, fig.width = 6)
library('tsoutliers')
library('ggplot2')
```

# Introduction

Data sets often contain observations that are far from other observations.
These outliers may be due to measurement error, or they may truly represent
anomalous or extreme behavior in the observed phenomenon. Outliers can have
a large influence in many statistical analyses, and since we prefer the results
of an analysis not to depend on a few unusual observations, it is necessary
to develop methods for avoiding or removing outlier effects.

With time series data, because of the time-dependent correlation structure
among the observations, outliers present unique problems and have to be dealt
with differently from outliers in i.i.d. observations. Suppose we want to model
a process as ARMA($p, q$). The data may contain measurement errors or may
have been affected by some unusual event(s). We would like a way to detect
outliers, which we might investigate more closely to determine their origin,
and adjust the series so that we can fit a model on what we believe the series
would have been without the measurement errors or unusual events.

We will consider four simple models for time series outliers and how they can
be detected by estimating their hypothetical effect at each time point using
linear regression. Large estimated effect at a given time point suggests an
outlier at that point. The estimate of outlier effect also allows us to remove
the effect and proceed to model the adjusted series. We will look at a
procedure developed by Chen and Liu (1993) to detect and estimate outlier
effects while also fitting an ARMA($p, q$) model. The R package `tsoutliers`
conveniently implements this procedure, and we will see examples of applying it
to real time series.

# Outlier Models

The following models describe four different ways outliers can enter into an
ARMA($p, q$) process $X_t$. For simplicity, we consider only non-seasonal,
non-integrated processes, though the models can be extended for seasonal
ARIMA processes. $X_t$ is described by

\begin{equation}
X_t = \frac{\theta(B)}{\phi(B)} Z_t 
\end{equation}

where $Z_t \sim_{iid}$ Normal$(0, \sigma^2)$. Now let $X^*_t$ denote the
observed series, which is contaminated by outliers. The outlier effect
on the series is $X^*_t - X_t$.

Consider two broad types of outlier effects. Either the effect is independent
of the underlying $X_t$ process, or the effect gets incorporated into the
process and is propagated through the "memory" described by $\phi(B)$ and
$\theta(B)$. Among the first type, the effect can be a simple impulse at one
time point---this is referred to as an additive outlier (AO)---or the effect
can decay over time---a temporary change (TC)---or remain permanently---a
level shift (LS). The second type, where the effect gets propagated by the
memory of the process, is basically equivalent to an impulse applied to the
"innovation" $Z_t$. More complicated models are possible, but these four in
combination can cover a large variety of cases.

## Additive Outlier (AO)

An additive outlier corresponds to an impulse on the series at one point,
where the impulse is external to the $X_t$ process. Large measurement errors
would clearly be additive outliers; the "impulse" would not represent anything
actually added to the $X_t$ process in that case.

The observed series given an AO at time $t = t_1$ is

\begin{equation}
X^*_t = X_t + \omega I_t(t_1)
\end{equation}

So the outlier effect is simply

\begin{equation}
X^*_t - X_t = \omega I_t(t_1)
\end{equation}

or $\omega$ at $t = t_1$ and zero elsewhere. The effect does not depend on
the ARMA process and has no lingering effects.

## Level Shift (LS)

A level shift is a sudden, persisent change in the mean of the time series,
or an external impulse that carries on into the future without diminishing.
An example might be a change in some performance measure for a company when
a new regulation goes into effect.

The observed series when a LS is present can be intuitively expressed as

\begin{equation}
X^*_t = X_t + \omega I_t(t \geq t_1)
\end{equation}

where $I_t(t \geq t_1) = 1$ for $t \geq t_1$, 0 otherwise. But an alternative
expression which will be more convenient later is

\begin{equation}
X^*_t = X_t + \frac{1}{1 - B} \omega I_t(t_1)
\end{equation}

The second form is easier to understand when rearranged to make the outlier
effect clear:
  
  \begin{equation}
(1 - B) (X^*_t - X_t) = \omega I_t(t_1)
\end{equation}

The application of $(1 - B)$ carries the effect over to all successive points
in the series. Note that again the effect does not depend on the ARMA process.

## Temporary Change (TC)

A temporary change is an abrupt change in the time series which decays over
time. For example, consider a time series recording daily
profit for a retail store. Now suppose there is some month long sporting sale
at the store which draws large crowds. The restaurant experiences inflated
sales due to the influx of people. It is likely that at the beginning of the
sale, a lot of people will visit the store. As the month progresses the total
sales per day should decrease, because many people have already gotten the
good deals. After the month ends, sales will go back to normal.

In this case the observed series is

\begin{equation}
X^*_t = X_t + \frac{1}{(1 - \delta B)} \omega I_t(t_1)
\end{equation}

where $0 \leq \delta \leq 1$. Again, this formulation is clearer when
rearranged:
  
  \begin{equation}
(1 - \delta B) (X^*_t - X_t) = \omega I_t(t_1)
\end{equation}

The outlier effect gets carried over but with some decay controlled by
$\delta$. Notice that the temporary change outlier is a generalization of the
AO and LS types. An AO is equivalent to a TC with $\delta = 0$, and a LS is a
TC with $\delta = 1$. The value of $\delta$ used to characterize TC outliers
in a particular setting must be chosen by the researcher. A suggested value
is $\delta = 0.7$.

## Innovational Outlier (IO)

The innovation (or "white noise") $Z_t$ can be thought of as combining
the effects of a large number of independent, unmeasured variables (plus,
                                                                    possibly, routine measurement error due to limited precision---distinct from
                                                                    the gross error that could result in an AO). Suppose that something causes an
impulse of $\omega$ to be added to the innovation at time $t_1$. (An example
                                                                  might be an earthquake creating an impulse in seismic activity.)
Then $X^*_t$ becomes

\begin{equation}
X^*_t = \frac{\theta(B)}{\phi(B)}\left[Z_t + \omega I_{t_1}(t)\right]
\end{equation}

where $I_{t_1}(t) = 1$ for $t = t_1$ and 0 otherwise. Rearranging and
substituting in $X_t$, we see that the outlier effect is

\begin{equation}
X^*_t - X_t = \frac{\theta(B)}{\phi(B)} \omega I_{t_1}(t)
\end{equation}

Thus, the outlier effect depends on the ARMA model through $\theta(B)$ and
$\phi(B)$. When an IO occurs at $t = t_1$, the effect of this outlier at
$t = t_1 + k$, $k \geq 0$, is

\begin{align}
X^*_{t_1 + k} - X_{t_1 + k} &= \frac{\theta(B)}{\phi(B)} \omega I_{t_1}(t_1 + k) \\
&= \psi(B) \omega I_{t_1}(t_1 + k) \\
&= \psi_k \omega
\end{align}

where $\psi(B) = \theta(B) / \phi(B)$ and $\psi_k$ is the $k^{th}$ coefficient
of $\psi(B)$, corresponding to the $B^k$ term. Because the time series is
stationary, the $\psi_k$ coefficients tend to zero as $k$ increases. This
implies that the IO effect diminishes and becomes zero given enough time. 

The various outlier effects are summarized by the following figure. The IO
effect is shown with four different ARMA models to demonstrate that the effect
depends on the model.

\vspace{0.5cm}

```{r fig.width = 6, fig.height = 5, fig.align = "center"}
# Calculate effect of IO given AR and MA coefficients, out to given lag
innov_outlier_effect <- function (ar, ma, lag) {
  ar <- ifelse(rep(is.null(ar), lag), rep(0, lag),
               c(ar, rep(0, lag - length(ar))))
  ma <- ifelse(rep(is.null(ma), lag), rep(0, lag),
               c(ma, rep(0, lag - length(ma))))
  effect <- numeric(lag)
  effect[1] <- 1
  for (k in 1:(lag - 1)) {
    effect[1 + k] <- ma[k]
    for (j in 1:k) {
      effect[1 + k] <- effect[1 + k] + ar[j] * effect[1 + k - j]
    }
  }
  effect
}

# Calculate and plot effects of each outlier type

par(mfrow = c(2, 2))
n <- 16
t <- 1:n
t1 <- 6 # Outlier here

# IO
ar <- c(-0.4, -0.2, 0.048)
ma <- c(-0.4, -0.2, 0.048)
io1 <- c(rep(0, t1 - 1), innov_outlier_effect(ar, ma, n - t1 + 1))
plot(io1, type = 'l', ylim = c(-1, 1), lty = 1,
     xlab = "Time", ylab = "Effect", main = "IO")
ar <- 0.5
ma <- c(0.4, 0.2, 0.048)
io2 <- c(rep(0, t1 - 1), innov_outlier_effect(ar, ma, n - t1 + 1))
lines(io2, lty = 2)
ma <- -0.7
io3 <- c(rep(0, t1 - 1), innov_outlier_effect(ar, ma, n - t1 + 1))
lines(io3, lty = 3)
ma <- NULL
io4 <- c(rep(0, t1 - 1), innov_outlier_effect(ar, ma, n - t1 + 1))
lines(io4, lty = 4)

# AO
ao <- rep(0, n)
ao[t1] <- 1
plot(ao, type = 'l', ylim = c(-0.1, 1),
     xlab = "Time", ylab = "Effect", main = "AO")

# LS
ls <- rep(0, n)
ls[t1:n] <- 1
plot(ls, type = 'l', ylim = c(-0.1, 1),
     xlab = "Time", ylab = "Effect", main = "LS")

# TC
tc <- rep(0, n)
tc[t1] <- 1
delta <- 0.7
for (k in (t1 + 1):n) {
  tc[k] <- delta * tc[k - 1]
}
plot(tc, type = 'l', ylim = c(-0.1, 1),
     xlab = "Time", ylab = "Effect", main = "TC")

```

# Outlier Detection and Estimation

In this section we consider the problem of detecting the presence of an
unknown number of outliers occuring at unknown times in a time series. The
outliers may be any of the four types described above. First, we describe
how the effects of a single outlier can be estimated using linear regression
Then we point out some practical issues, and the section culminates in a
description of the procedure proposed by Chen and Liu (1993) to jointly
estimate model parameters and outlier effects in a series.

## Estimating the Effect of a Single Outlier

Suppose we have an observed time series $X^*_t$ that arose from an ARMA($p, q$)
process $X_t$ with one outlier at time $t_1$. For now, assume we know the ARMA
model parameters, $t_1$, and the type of outlier. We will find an estimate
of the outlier magnitude $\omega$. (Later we discuss how this calculation is
                                    useful when the locations and types of outliers are unknown.)

\begin{equation}
\pi(B) = \frac{\phi(B)}{\theta(B)} = 1 - \pi_1B - \pi_2B^2 - \pi_3B^3 - ...
\end{equation}

Since $X_t = \frac{\theta(B)}{\phi(B)} Z_t$, applying $\pi(B)$ to $X_t$ would
result in the purely random residuals, $Z_t$. But $X^*_t$ is contaminated
by the outlier, so the residuals that result from applying $\pi(B)$ to
$X^*_t$ are also contaminated. Let $\hat{e}_t = \pi(B) X^*_t$ be those
contaminated residuals. For the different outlier types, we have

\begin{align}
\text{IO: } \hat{e}_t &= \omega I_t(t_1) + Z_t \\
\text{AO: } \hat{e}_t &= \omega \pi(B) I_t(t_1) + Z_t \\
\text{LS: } \hat{e}_t &= \omega \frac{\pi(B)}{1 - B} I_t(t_1) + Z_t \\
\text{TC: } \hat{e}_t &= \omega \frac{\pi(B)}{1 - \delta B} I_t(t_1) + Z_t
\end{align}

These equations can be put in the form of a linear regression where the
responses are the residuals $\hat{e}_t$ and the predictors are different
for each type of outlier. The outlier magnitude $\omega$ is the coefficient
to be estimated:
  
  \begin{equation}
\hat{e}_t = \omega x_t + Z_t
\end{equation}

For all outlier types, $x_t = 0$ for $t < t_1$ and $x_t = 1$ for $t = t_1$.
For $t = t_1 + k$, $k \geq 1$, the value of $x_t$ depends on the outlier type:
  
  \begin{align}
\text{IO: } x_{t_1 + k} &= 0 \\
\text{AO: } x_{t_1 + k} &= -\pi_k \\
\text{LS: } x_{t_1 + k} &= 1 - \sum_{j = 1}^{k} \pi_j \\
\text{TC: } x_{t_1 + k} &= \delta^k -
  \sum_{j = 1}^{k - 1} \delta^{k - j} \pi_j - \pi_k
\end{align}

The least-squares estimate for $\omega$ can then be calculated for each
type of outlier, using the appropriate $x_t$ values:
  
  \begin{equation}
\hat{\omega} = \frac{\sum_{t = t_1}^{n} \hat{e}_t x_{t}}{\sum_{t = t_1}^{n} x^2_{t}}
\end{equation}

Note that when $t_1 = n$ (the last observation in the time series), it is
impossible to distinguish the different types of outliers, as $x_{t_1} = 1$
  for all types, and this is the only predictor value available.

Once we have an estimate of $\omega$ corresponding to a particular type of
outlier at $t = t_1$, we can use equations (2, 7, 10, 12) to remove the outlier
effect and obtain and adjusted series which estimates $X_t$, the uncontaminated
process.

## Using the Single-Outlier Estimate

For the estimation of $\omega$, we assumed the ARMA model parameters and the
outlier type and location $t_1$ were known. In practice, they are not. The model
parameters must first be estimated, then $\hat{\omega}$ can be calculated
for each type of outlier at each $t_1 = 1, \dots, n$. If $\hat{\omega}$ is large
at some $t_1$, and largest at $t_1$ for one type of outlier, it might be
justified to say there is an outlier of that type at $t_1$. This raises two
issues.

First, estimates for the model parameters will be influenced by outliers.
Eventually we want parameter estimates uninfluenced by outliers, but we have
to start somehow. Cycling through estimation of model parameters, detection
of outliers, and adjustment of the series to remove outlier effects seems a
reasonable strategy. This is what procedure described below does.

The other issue is that the $\hat{\omega}$ values are calculated using
different sets of predictors and responses in the regression, so they cannot be
compared immediately. Dividing $\hat{\omega}$ by its standard error results in
standardized statistics which are approximately normally distributed:
  
  \begin{equation}
\hat{\tau} = \frac{\hat{\omega}}{\sqrt{\hat{\sigma}_a^2 / \sum_{t = t_1}^{n} x^2_t}}
\end{equation}

where $\hat{\sigma}_a^2$ is an estimate of the variance of $Z_t$. The $\hat{\tau}$ statistics allow for
comparison among various outlier types at various times.

## Effects of Multiple Outliers

Estimation of outlier effects and calculation of statistics as described
above becomes complicated when multiple outliers are present. When it is assumed
(as above) that only one outlier is present, but in fact there are many, the
presence of the other outliers may bias the estimate of the effect of one
at the time under consideration. Or the effects of an outlier may
be masked by the effects of an earlier outlier.

The problem of bias and masking suggests that the effects of all of the outliers
should be estimated jointly. But doing so when the number and time locations
of the outliers are unknown would require much more complex calculations.
The procedure described below detects outliers one at a time but attempts to
solve the bias and masking problem by repeatedly adjusting and re-examining
both the time series and the set of identified outliers.

## Estimating $\sigma_a$

The $\hat{\tau}$ statistic requires an estimate of $\sigma_a$. Since we
are assuming the presence of outliers which could bias the estimate upwards,
it is important to use an estimate of $\sigma_a$ that is robust to outliers.
Three options are the median absolute deviation (MAD), a trimmed standard
deviation, and the usual standard deviation of residuals calculated with the
residual at time $t_1$ omitted.

## Detection and Estimation Procedure

Chen and Liu (1993) proposed an iterative procedure for detecting outliers,
estimating their effects, and estimating the parameters of an ARMA model
to fit the series after adjusting for outlier effects. The procedure assumes
that the order of the ARMA model is known. We will see that implementation
of this procedure in `tsoutliers` can also automatically choose the order of
the ARMA model.

The procedure has three stages. In Stage I, outliers are accumulated one by one
in order of descending magnitude of $\hat{\tau}$ statistic. In Stage II, any
outliers that can no longer be considered outliers after taking into account
the effects of all the others are dropped. In Stage III, the model estimates
are fixed at their final values and one last round of outlier accumulation-
  elimination results in the final set of outliers and estimates of their effects.

### Stage I

1. Compute the maximum likelihood estimates of the model parameters.
2. For $t = 1, \dots, n$ calculate $\hat{\tau}$ for each type of outlier.
Let $\hat{\tau}_{max}$ be the maximum of the absolute values of the $\hat{\tau}$
  statistics. If $\hat{\tau}_{max} > C$, where $C$ is a constant chosen beforehand,
there is (potentially) an outlier at the time and of the type for which
$\hat{\tau}_{max}$ occurred. Remove the effect of the outlier from the series.
3. Repeat (2) until no more $\hat{\tau}$ statistics indicate potential outliers.
4. If any outliers have been found in (2--3), go back to (1), computing
the parameter estimates on the series that has now been adjusted for outlier
effects, and iterate through (2--3) again. If no outliers were found, proceed
to Stage II.

### Stage II

5. Jointly estimate the outlier effects using a multiple regression
based on (24). Calculate the $\hat{\tau}$ statistics. If any of the statistics
no longer exceed the threshold $C$, drop the corresponding outlier.
6. Repeat (5) until no more outliers are dropped.
7. Obtain jointly estimated outlier effects using only the remaining outliers
and adjust the original series by removing their effects.
8. Compute the maximum likelihood estimates of the model parameters using
the adjusted series. If the change in residual standard error from the
previous estimate exceeds $\epsilon$, another threshold constant chosen
beforehand, go back to (5), using the new parameter estimates.
Otherwise, proceed to Stage III.

### Stage III

9. The parameter estimates found in (8) are the final estimates for the model.
Using only these model parameters, iterate through (2--3) and (5--6) to obtain
a final set of outliers and estimates of their effects.

### Comments

The procedure uses two threshold values, $C$ and $\epsilon$, which are chosen
by the user. Based on simulations, Chen and Liu recommend $C = 2.5$
  to 2.9 for a shorter series of length less than $n = 100$ or so, and $C = 3.0$
  for $n = 100$ to 200. For longer series, $n > 200$, $C$ greater than 3.0 may
be appropriate, but in any case multiple values should be tried to assess
how sensitive the results are to $C$. The value of $\epsilon$ controls the
accuracy of the parameter estimates, and a suggested value is 0.001.

# Examples

### R package`tsoutliers` 
Following the approach described in Chen & Liu (1993), an automatic procedure for detection of outliers in time series is implemented in the package tsoutliers. This package implements a procedure based on the approach described in Chen and Liu (1993) for automatic detection of outliers in time series. Innovational outliers, additive outliers, level shifts,
temporary changes and seasonal level shifts are considered. 

The function `tso` is the main interface for the automatic procedure. The functions `locate.outliers.oloop`
and `remove.outliers` implement respectively the first and second stages of the procedure. In practice,
the user may stick to use the function `tso`. All the options at any stage of the procedure can be defined through the arguments passed to `tso`. (CRAN documentation)

`tso(y, xreg = NULL, cval = NULL, delta = 0.7, n.start = 50,
     types = c("AO", "LS", "TC"),
     maxit = 1, maxit.iloop = 4, cval.reduce = 0.14286,
     remove.method = c("en-masse", "bottom-up"),
     remove.cval = NULL,
     tsmethod = c("auto.arima", "arima", "stsm"),
     args.tsmethod = NULL, args.tsmodel = NULL, logfile = NULL)
`


[*Possible questions to address, in addition to showing one or two examples:*]

* What functions does the `tsoutliers` package offer? What exactly do they do?
In particular, how exactly is `auto.arima` used within `tso`?
* What arguments need to be specified? What are the defaults? When should one
provide arguments other than the defaults?
* How does one decide which types of outliers to look for?
* How does one choose $C$ and $\delta$?
* What is the output of the R function(s) and how does one interpret it?

#### Example 1:  Outlier in measurement of the annual flow of the river Nile at Ashwan from 1871-1970.

Fitting auto.arima model gives an ARIMA(1,1,1).

```{r,fig.align='left',echo=TRUE,eval=TRUE}
library(fma)

data(Nile)
plot(Nile)
fit<- auto.arima(Nile)
#fits with ARIMA(1,1,1)
print(fit)
```

Applying the two stage outlier detection process using `tsoutliers` function
generates

```{r,eval=TRUE,echo=TRUE}
nile.outliers <- tso(Nile,types=c("AO","LS","TC", "IO"))
plot(nile.outliers)
```
The outlier detected are 
```{r, echo=TRUE,results='markup'}
# Outlier values 
print(nile.outliers$outliers)
```
The main detected outliers are `level shift (LS)` in 1899 and `Additive outlier (AO)` in 1913.  Additinaly, the fitted ARIMA model is ARIMA( 0,0,0).

```{r, echo=TRUE,results='markup'}
# Outlier values 
print(nile.outliers$fit)
```
One can explicitly obtain the outlier effects at each time point from the effects argument after running the tsoutliers command.  From there, you can obtain the outlier adjusted time series taking the original time series and subtracting these effects.

```{r, echo=TRUE,results='markup'}
plot(Nile - nile.outliers$effects, xlab = "Time", ylab = "River Flow", main = "Outlier Adjusted Time Series")
```

#### Example 2. Outliers in price of chicken in USA from 1924 to 1993. 
The data shows the price was gradually decreasing, but there might be an outliers hidden in the trends. 
```{r,echo=TRUE}
library(fma)
plot(chicken,main="Chicken price in USA from 1924 to 1993")

```
Applying `'tsoutliers::tso` for 30 iteration, identifies the following outliers. 

```{r,echo=TRUE}
#check outliers for 30 maximum iteration
chicken.outlier <- tsoutliers::tso(chicken,types=c("AO","LS","TC","IO"),maxit.iloop=30)
```
The fitted ARIMA model  
```{r, echo=TRUE,results='markup'}
print(chicken.outlier$fit)
```
Detectd outliers
```{r, echo=TRUE,results='markup'}
# Outlier values 
plot(chicken.outlier)
#Summary of the detected outliers
print(chicken.outlier$outliers)
```
As we can see the main two outliers are `level shift outlier` in 1935. and `temporary change` outlier in 1943. 

```{r, echo=TRUE,results='markup'}
plot(chicken - chicken.outlier$effects, xlab = "Time", ylab = "River Flow", main = "Outlier Adjusted Time Series")
```

# References

Chang, I., Tiao, G. C., and Chen, C. (1988), "Estimation of Time Series
Parameters in the Presence of Outliers," *Technometrics*, 30, 193--204.

Chen, C. and Liu, Lon-Mu (1993), "Joint Estimation of Model Parameters and
Outlier Effects in Time Series," *Journal of the American Statistical
Association*, 88, 284--297.

Chen, C., and Tiao, G. C. (1990), "Random Level Shift Time Series Models,
ARIMA Approximation, and Level Shift Detection," *Journal of Business and
Economic Statistics*, 8, 170--186.

Fox, A. J. (1972), "Outliers in Time Series," *Journal of the Royal Statistical
Society*, Ser. B, 34, 350--363.

Tsay, Ruey S. (1986), "Time Series Model Specification in the Presence of
Outliers," *Journal of the American Statistical Association*, 81, 132--141.

Galeano,Pena (2013) "Finding Outliers in Linear and Nonlinear Time Series" Chapter
Robustness and Complex Data Structures pp 243-260

https://cran.r-project.org/web/packages/tsoutliers/tsoutliers.pdf

# Appendix

```{r, all-code, ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}

```
